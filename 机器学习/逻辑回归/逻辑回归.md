# <center>逻辑回归</center>



![1568212877880](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568212877880.png)



![1568213239721](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568213239721.png)

![1568213265539](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568213265539.png)





## sigmoid 函数



```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(t):
    return 1. / (1. + np.exp(-t))

x = np.linspace(-10, 10, 500)

plt.plot(x, sigmoid(x))
plt.show()
```

![1568213201890](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568213201890.png)



 



![1568214479343](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214479343.png)



![1568214513917](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214513917.png)





![1568214669618](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214669618.png)

根据上述要求，需要构造一个满足这种特性的损失函数。



![1568214686852](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214686852.png)



![1568214852357](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214852357.png)



![1568214920234](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214920234.png)



![1568215012036](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568215012036.png)





## 实现逻辑回归

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target

X = X[y<2,:2]
y = y[y<2]

plt.scatter(X[y==0,0], X[y==0,1], color="red")
plt.scatter(X[y==1,0], X[y==1,1], color="blue")
plt.show()
```

![1568904558579](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568904558579.png)



逻辑回归封装类

```python
import numpy as np
from .metrics import accuracy_score

class LogisticRegression:

    def __init__(self):
        """初始化Logistic Regression模型"""
        self.coef_ = None
        self.intercept_ = None
        self._theta = None

    def _sigmoid(self, t):
        return 1. / (1. + np.exp(-t))

    def fit(self, X_train, y_train, eta=0.01, n_iters=1e4):
        """根据训练数据集X_train, y_train, 使用梯度下降法训练Logistic Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        def J(theta, X_b, y):
            y_hat = self._sigmoid(X_b.dot(theta))
            try:
                return - np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) / len(y)
            except:
                return float('inf')

        def dJ(theta, X_b, y):
            return X_b.T.dot(self._sigmoid(X_b.dot(theta)) - y) / len(y)

        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

            theta = initial_theta
            cur_iter = 0

            while cur_iter < n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
                    break

                cur_iter += 1

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict_proba(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果概率向量"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return self._sigmoid(X_b.dot(self._theta))

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        proba = self.predict_proba(X_predict)
        return np.array(proba >= 0.5, dtype='int')

    def score(self, X_test, y_test):
        """根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(X_test)
        return accuracy_score(y_test, y_predict)

    def __repr__(self):
        return "LogisticRegression()"
```



### 使用逻辑回归

```python
from playML.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, seed=666)

from playML.LogisticRegression import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

log_reg.score(X_test, y_test)
```

> ```
> 1.0
> ```

```python
log_reg.predict_proba(X_test)
```

> ```
> array([ 0.92972035,  0.98664939,  0.14852024,  0.17601199,  0.0369836 ,
>         0.0186637 ,  0.04936918,  0.99669244,  0.97993941,  0.74524655,
>         0.04473194,  0.00339285,  0.26131273,  0.0369836 ,  0.84192923,
>         0.79892262,  0.82890209,  0.32358166,  0.06535323,  0.20735334])
> ```

```python
log_reg.predict(X_test)

y_test
```

> ```
> array([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])
> 
> array([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])
> ```



![1568905352688](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905352688.png)

 

```python
log_reg.coef_
#  array([ 3.01796521, -5.04447145])

log_reg.intercept_
#  -0.6937719272911228
```

```python
def x2(x1):
    return (-log_reg.coef_[0] * x1 - log_reg.intercept_) / log_reg.coef_[1]

x1_plot = np.linspace(4, 8, 1000)
x2_plot = x2(x1_plot)

plt.scatter(X[y==0,0], X[y==0,1], color="red")
plt.scatter(X[y==1,0], X[y==1,1], color="blue")
plt.plot(x1_plot, x2_plot)
plt.show()
```

![1568905531374](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905531374.png)

```python
# 只绘制测试数据
plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1], color="red")
plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1], color="blue")
plt.plot(x1_plot, x2_plot)
plt.show()
```

![1568905618985](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905618985.png)





![1568905730081](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905730081.png)

![1568905722899](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905722899.png)



```python
# 封装绘制边界函数
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
```

```Python
plot_decision_boundary(log_reg, axis=[4, 7.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1568905819871](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905819871.png)



### kNN的决策边界

```python
from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)

knn_clf.score(X_test, y_test)
# 1.0
```

```python
plot_decision_boundary(knn_clf, axis=[4, 7.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1568905888570](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905888570.png)



```python
knn_clf_all = KNeighborsClassifier()
knn_clf_all.fit(iris.data[:,:2], iris.target)

plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])
plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1])
plt.show()
```

![1568906099429](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568906099429.png)

  从图中黄色边界形状可以看出发生了过拟合



```python
knn_clf_all = KNeighborsClassifier(n_neighbors=50)
knn_clf_all.fit(iris.data[:,:2], iris.target)

plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])
plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1])
plt.show()
```

![1568906207160](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568906207160.png)





![1569050181912](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569050181912.png)





![1569050242107](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569050242107.png)



## 逻辑回归中添加多项式特征

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.normal(0, 1, size=(200, 2))
y = np.array((X[:,0]**2+X[:,1]**2)<1.5, dtype='int')

plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1569050390805](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569050390805.png)



### 使用逻辑回归

```python
from playML.LogisticRegression import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X, y)
log_reg.score(X, y)
```

> ```
> 0.60499999999999998
> ```

```python
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
```

```python
plot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1569050524549](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569050524549.png)

相当于是使用的一条直线对逻辑回归分类平面进行划分。



```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

def PolynomialLogisticRegression(degree):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression())
    ])
```

```python
poly_log_reg = PolynomialLogisticRegression(degree=2)
poly_log_reg.fit(X, y)

poly_log_reg.score(X, y)
```

> ```
> 0.94999999999999996
> ```

```python
plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1569050645571](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569050645571.png)

degree 取太大的值，发生过拟合

```python
poly_log_reg2 = PolynomialLogisticRegression(degree=20)
poly_log_reg2.fit(X, y)

plot_decision_boundary(poly_log_reg2, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1569051364671](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569051364671.png)



逻辑回归中使用正则化

![1569051520972](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569051520972.png)



## scikit-learn中的逻辑回归

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.normal(0, 1, size=(200, 2))
y = np.array((X[:,0]**2+X[:,1])<1.5, dtype='int')
for _ in range(20):		# 随机挑20个点，强制变成1
    y[np.random.randint(200)] = 1	# 相当于添加噪音
    
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1569051695038](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569051695038.png)



```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
```

```python
log_reg.score(X_train, y_train)
```

> ```
> 0.79333333333333333
> ```

```python
log_reg.score(X_test, y_test)
```

> ```
> 0.85999999999999999
> ```

```python
plot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```



使用多项式逻辑回归

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

def PolynomialLogisticRegression(degree):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression())
    ])

poly_log_reg = PolynomialLogisticRegression(degree=2)
poly_log_reg.fit(X_train, y_train)
```

```python
poly_log_reg.score(X_train, y_train)
```

> ```
> 0.91333333333333333
> ```

```python
poly_log_reg.score(X_test, y_test)
```

> ```
> 0.93999999999999995
> ```

```python
plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1569051881257](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569051881257.png)



加大 degree 的值，产生过拟合

```python
poly_log_reg2 = PolynomialLogisticRegression(degree=20)
poly_log_reg2.fit(X_train, y_train)
```

```python
poly_log_reg2.score(X_train, y_train)
```

> ```
> 0.93999999999999995
> ```

```python
poly_log_reg2.score(X_test, y_test)
```

> ```
> # 准确率有所降低
> 0.92000000000000004
> ```

```python
plot_decision_boundary(poly_log_reg2, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1569051973651](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569051973651.png)





加入正则化的逻辑回归

```python
def PolynomialLogisticRegression(degree, C):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C=C))
    ])

poly_log_reg3 = PolynomialLogisticRegression(degree=20, C=0.1) # 此时正则化项作业大
poly_log_reg3.fit(X_train, y_train)
```

```python
poly_log_reg3.score(X_train, y_train)
```

> ```
> 0.85333333333333339
> ```

```python
poly_log_reg3.score(X_test, y_test)
```

> ```
> 0.92000000000000004
> ```

```python
plot_decision_boundary(poly_log_reg3, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1569052057665](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569052057665.png)



使用 L2 正则项

```python
def PolynomialLogisticRegression(degree, C, penalty='l2'):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C=C, penalty=penalty))
    ])

poly_log_reg4 = PolynomialLogisticRegression(degree=20, C=0.1, penalty='l1')
poly_log_reg4.fit(X_train, y_train)
```

```python
poly_log_reg4.score(X_train, y_train)
```

> ```
> 0.82666666666666666
> ```

```python
poly_log_reg4.score(X_test, y_test)
```

> ```
> 0.90000000000000002
> ```

```python
plot_decision_boundary(poly_log_reg4, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1569052194272](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569052194272.png)





![1569052504764](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569052504764.png)



![1569052599251](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569052599251.png)





![1569052701282](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569052701282.png)



## OvR 和 OvO



```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

iris = datasets.load_iris()
X = iris.data[:,:2]
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

log_reg.score(X_test, y_test)
```

![1569052843010](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569052843010.png)

可以看出，sklearn 中默认使用 'ovr' 的方式

```python
plot_decision_boundary(log_reg, axis=[4, 8.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
```

![1569052884146](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569052884146.png)



使用OVO方式实现多分类，需要设置参数 “multinomial” 方式实现多分类（同时设置solver 计算方式参数）

```python
log_reg2 = LogisticRegression(multi_class="multinomial", solver="newton-cg")
log_reg2.fit(X_train, y_train)
log_reg2.score(X_test, y_test)
```

> ```
> 0.78947368421052633
> ```

```python
plot_decision_boundary(log_reg2, axis=[4, 8.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
```

![1569052929086](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1569052929086.png)





### 使用所有的数据

```python
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
```

> ```
> 0.94736842105263153
> ```

```python
log_reg2 = LogisticRegression(multi_class="multinomial", solver="newton-cg")
log_reg2.fit(X_train, y_train)
log_reg2.score(X_test, y_test)
```

> ```
> 1.0
> ```



### OvO and OvR

sklearn 封装的逻辑回归提供了 ovo 和 ovr 实现多分类，同时 sklearn 还提供了两个类，专门完成多分类任务。

```python
from sklearn.multiclass import OneVsRestClassifier

ovr = OneVsRestClassifier(log_reg)
ovr.fit(X_train, y_train)
ovr.score(X_test, y_test)
```

> ```
> 0.94736842105263153
> ```

```python
from sklearn.multiclass import OneVsOneClassifier

ovo = OneVsOneClassifier(log_reg)
ovo.fit(X_train, y_train)
ovo.score(X_test, y_test)
```

> ```
> 1.0
> ```





