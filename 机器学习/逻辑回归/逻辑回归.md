# <center>逻辑回归</center>



![1568212877880](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568212877880.png)



![1568213239721](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568213239721.png)

![1568213265539](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568213265539.png)





## sigmoid 函数



```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(t):
    return 1. / (1. + np.exp(-t))

x = np.linspace(-10, 10, 500)

plt.plot(x, sigmoid(x))
plt.show()
```

![1568213201890](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568213201890.png)







![1568214479343](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214479343.png)



![1568214513917](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214513917.png)





![1568214669618](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214669618.png)

根据上述要求，需要构造一个满足这种特性的损失函数。



![1568214686852](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214686852.png)



![1568214852357](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214852357.png)



![1568214920234](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568214920234.png)



![1568215012036](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568215012036.png)





## 实现逻辑回归

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target

X = X[y<2,:2]
y = y[y<2]

plt.scatter(X[y==0,0], X[y==0,1], color="red")
plt.scatter(X[y==1,0], X[y==1,1], color="blue")
plt.show()
```

![1568904558579](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568904558579.png)



### 使用逻辑回归

```python
from playML.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, seed=666)

from playML.LogisticRegression import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

log_reg.score(X_test, y_test)
```

> ```
> 1.0
> ```

```python
log_reg.predict_proba(X_test)
```

> ```
> array([ 0.92972035,  0.98664939,  0.14852024,  0.17601199,  0.0369836 ,
>         0.0186637 ,  0.04936918,  0.99669244,  0.97993941,  0.74524655,
>         0.04473194,  0.00339285,  0.26131273,  0.0369836 ,  0.84192923,
>         0.79892262,  0.82890209,  0.32358166,  0.06535323,  0.20735334])
> ```

```python
log_reg.predict(X_test)

y_test
```

> ```
> array([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])
> 
> array([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])
> ```



![1568905352688](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905352688.png)

 

```python
log_reg.coef_
#  array([ 3.01796521, -5.04447145])

log_reg.intercept_
#  -0.6937719272911228
```

```python
def x2(x1):
    return (-log_reg.coef_[0] * x1 - log_reg.intercept_) / log_reg.coef_[1]

x1_plot = np.linspace(4, 8, 1000)
x2_plot = x2(x1_plot)

plt.scatter(X[y==0,0], X[y==0,1], color="red")
plt.scatter(X[y==1,0], X[y==1,1], color="blue")
plt.plot(x1_plot, x2_plot)
plt.show()
```

![1568905531374](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905531374.png)

```python
# 只绘制测试数据
plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1], color="red")
plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1], color="blue")
plt.plot(x1_plot, x2_plot)
plt.show()
```

![1568905618985](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905618985.png)





![1568905730081](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905730081.png)

![1568905722899](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905722899.png)



```python
# 封装绘制边界函数
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
```

```Python
plot_decision_boundary(log_reg, axis=[4, 7.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1568905819871](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905819871.png)



### kNN的决策边界

```python
from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)

knn_clf.score(X_test, y_test)
# 1.0
```

```python
plot_decision_boundary(knn_clf, axis=[4, 7.5, 1.5, 4.5])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.show()
```

![1568905888570](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568905888570.png)



```python
knn_clf_all = KNeighborsClassifier()
knn_clf_all.fit(iris.data[:,:2], iris.target)

plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])
plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1])
plt.show()
```

![1568906099429](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568906099429.png)

  从图中黄色边界形状可以看出发生了过拟合



```python
knn_clf_all = KNeighborsClassifier(n_neighbors=50)
knn_clf_all.fit(iris.data[:,:2], iris.target)

plot_decision_boundary(knn_clf_all, axis=[4, 8, 1.5, 4.5])
plt.scatter(iris.data[iris.target==0,0], iris.data[iris.target==0,1])
plt.scatter(iris.data[iris.target==1,0], iris.data[iris.target==1,1])
plt.scatter(iris.data[iris.target==2,0], iris.data[iris.target==2,1])
plt.show()
```

![1568906207160](E:\MardkDown-Books\机器学习\逻辑回归\逻辑回归.assets\1568906207160.png)