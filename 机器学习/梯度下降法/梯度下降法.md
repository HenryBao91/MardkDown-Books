# <center>梯度下降法</center>

## 1、梯度下降法

## 1.1、梯度下降

​		梯度下降是一种非常通用的<font color='red'>优化算法</font>，能够为大范围的问题寻找最优解。**梯度下降的中心思想就是：迭代地调整参数从而使成本函数最小化**。

特点：

- 梯度下降法不是一个机器学习算法

- 梯度下降法是一种基于搜索的最优化方法

- 作用：最小化一个损失函数

- 梯度上升法：最大化一个效用函数

  使用梯度下降方法的原因：**很多机器学习的模型是无法直接求到最优解**。

![1563885114058](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563885114058.png)

​		对于上图损失函数来说，在损失函数上任取一点（下图蓝点），可以得到该点的损失函数 $J$ 值和该点的导数。由数学知识可知，如果该点的导数不为0，则该点肯定不在函数的极值点上，导数通常可以写为$\frac{dJ}{d\theta}$。

![1563885900707](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563885900707.png)

​		对于该问题，导数代表 $\theta$ 变化时，$J$ 相应的变化。导数可以代表方向，对应 $J$ 增大的方向，由于导数是负值，所以 $J$ 增大的方向应该是在 $\theta$ 轴的负方向上，也就是 $\theta$ 减小的时候。对应的还需要一个  $\theta$ 移动的步长，即 $\eta$ ，梯度下降中一个重要参数是每一步的步长，这取决于超参数学习率。叫做梯度下降法的原因：对于一维的的可以直接求导表征，但是对于多维的需要求各个方向的导数，也就是函数的梯度。

## 1.2、梯度下降法关键点

### 1.2.1、学习率

​		对于 $\eta$ ：

-   $\eta$ 称为学习率（Learning rate）
-   $\eta$ 的取值影响获得最优解的速度
-   $\eta$ 取值不合适，甚至得不到最优解
-   $\eta$ 是梯度下降法的一个超参数

1. 学习率 $\eta$ 太小，则算法需要经过大量迭代才能收敛，这将耗费很长时间。

![1563886481781](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563886481781.png)



2. 学习率 $\eta$ 太高，则算法可能会发散，值越来越大，最后无法找到好的解决方案。

![1563886552844](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563886552844.png)



### 1.2.2、梯度陷阱

​		对于线性回归算法而言，线性回归模型的 MSE 成本函数恰好是一个凸函数，这意味着连接曲线上任意两个点的线段永远不会跟曲线相交，即不存在局部最小，只存在一个全局最小值。

![1563886936546](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563886936546.png)

​		**并不是对于所有的函数都有唯一的极值点**。

 解决方法：

- 多次运行，随机初始化点
- 梯度下降法的初始点也是一个超参数

### 1.2.3、特征值缩放

​		即使损失函数是凸函数，但如果不同特征的尺寸差别巨大，如下图，那它可能是一个非常细长的“碗”，图中左边训练集上特征1和特征2具有相同的数值规模，而右边的训练集上，特征1的值则比特征2小得多。

![1563887217478](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563887217478.png)

​		由此可见，左图的梯度下降法直接走向最小值，可以快速到达。而在右图中，先是沿着与全局最小值方向近乎垂直的方向前进，接下来是一段几乎平坦的尝尝的山谷，最终虽然也能够到达最小值，但是需要花费大量的训练时间。

​		<font color='darkblue'>**应用梯度下降法时，需要保证所有特征值的大小比例差不多，可以使用sklearn中的StandartScaler类，否则收敛的时间会很长。**</font>



## 2、梯度下降法模拟

### 2.1、梯度下降法实验

```python
import numpy as np
import matplotlib.pyplot as plt

plot_x = np.linspace(-1, 6, 141)
plot_y = (plot_x - 2.5)**2 - 1   # y 是 x 的简单二次函数

plt.plot(plot_x, plot_y) 
plt.show()
```

图像：

![1563887852853](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563887852853.png)

```python
# 函数导数：
def dJ(theta):
    return 2*(theta - 2.5)

# 损失函数：
def J(theta):
    return (theta - 2.5)**2 - 1

eta = 0.1
epsilon = 1e-8

theta = 0.0
while True:
    grandient = dJ(theta)  # 梯度
    last_theta = theta     # 当前 theta
    theta = theta - eta * grandient  # 新移动的 theta
    # 判断 theta 是否来到了导数最小值的点
    if( abs(J(theta) - J(last_theta)) < epsilon ):  
        break

print(theta)
print(J(theta))
```

> output:
>
> > ```python
> > 2.499891109642585
> > -0.99999998814289
> > ```

对 $\theta$ 的观察：

```python
theta = 0.0
theta_history = [theta]
while True:
    gradient = dJ(theta)
    last_theta = theta
    theta = theta - eta * gradient
    theta_history.append(theta)
    
    if(abs(J(theta) - J(last_theta)) < epsilon):
        break

plt.plot(plot_x, J(plot_x))
plt.plot(np.array(theta_history), J(np.array(theta_history)), color="r", marker='+')
plt.show() 
```

绘制图像：

![1563888291454](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563888291454.png)



### 2.2、梯度下降法封装

```python
theta_history = []

def gradient_descent(initial_theta, eta, epsilon=1e-8):
    theta = initial_theta
    theta_history.append(initial_theta)

    while True:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
    
        if(abs(J(theta) - J(last_theta)) < epsilon):
            break
            
def plot_theta_history():
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color="r", marker='+')
    plt.show()
```

1. 减小$\theta$ 观察输出效果：

```python
eta = 0.01
theta_history = []
gradient_descent(0, eta)
plot_theta_history()
```

​	绘制输出图像（$\theta$ 更加密集）：

![1563888530813](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563888530813.png)

2. 增大$\theta$ 观察输出效果：

```python
eta = 0.8
theta_history = []
gradient_descent(0, eta)
plot_theta_history()
```

​	绘制输出图像（$\theta$ 更加稀疏，幅度更大）：

![1563888697463](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563888697463.png)

3. 继续增大$\theta$ 观察输出效果：

```python
eta = 1.1   #  eta 设置的过大
theta_history = []
gradient_descent(0, eta)
plot_theta_history()
```

​	编译器报错：

![1563888766216](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563888766216.png)

修改损失函数和梯度下降法代码，避免报错：

```PYTHON
def J(theta):
    try:
        return (theta-2.5)**2 - 1.
    except:
        return float('inf')   # 进行异常检测，如果错误返回浮点数最大值
    
 #  n_iters 代表运行最大循环次数  
def gradient_descent(initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
    
    theta = initial_theta
    i_iter = 0
    theta_history.append(initial_theta)

    while i_iter < n_iters:    # 避免死循环
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
    
        if(abs(J(theta) - J(last_theta)) < epsilon):
            break
            
        i_iter += 1
        
    return    
```

重新执行上面代码：

```PYTH
eta = 1.1
theta_history = []
gradient_descent(0, eta, n_iters=10)
plot_theta_history()
```

输出图像：

![1563889011407](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563889011407.png)



# 3、多元回归中的梯度下降法

![1563974316749](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563974316749.png)

​	对于多元线性回归来说，$\theta$ 需要用向量来表示，代表多个参数；对于最简单的一元线性回归来说，$\theta$ 也包含有两个值，即 $\theta = (\theta_0, \theta_1)$ ，相当于 $y = \theta_0 + \theta_1\cdot x $ 。此时，损失函数为梯度为：
$$
\nabla J = (\frac{\partial J}{\partial \theta_0}, \frac{\partial J}{\partial \theta_1}, ... , \frac{\partial J}{\partial \theta_n})
$$

### 3.1、梯度下降法的推导

​	**目标**：使 $\sum\limits^m_{i=1}(y^{(i)} - \hat y^{(i)} )^2 $ 尽可能的小。

​	其中，$\hat y^{(i)}  = \theta_0 + \theta_1 \mathbf X_1^{(i)} + \theta_2 \mathbf X_2^{(i)} + ... + + \theta_n \mathbf X_n^{(i)}$ 。

​	所以，求解目标可以转换为： $\sum\limits^m_{i=1}(y^{(i)} -  \theta_0 + \theta_1 \mathbf X_1^{(i)} + \theta_2 \mathbf X_2^{(i)} + ... + + \theta_n \mathbf X_n^{(i)} )^2 $ 尽可能小。由于此时求解的变量是 $\theta$ , 所以求导过程中为了直观交换一下顺序，同时把 $\hat y^{(i)}$ 写成向量形式。
$$
\nabla J(\theta) = 
\begin{pmatrix}
\frac{\partial J}{\partial \theta_0}   \\
\frac{\partial J}{\partial \theta_1}    \\
\frac{\partial J}{\partial \theta_2}    \\
\vdots   \\
\frac{\partial J}{\partial \theta_n}   \\
\end{pmatrix}
= 
\begin{pmatrix}
\sum\limits^m_{i=1}2(y^{(i)} - \mathbf X_b^{(i)}\theta  )\cdot(-1)  \\
\sum\limits^m_{i=1}2(y^{(i)} - \mathbf X_b^{(i)}\theta  )\cdot(-X_1^{(i)})  \\
\sum\limits^m_{i=1}2(y^{(i)} - \mathbf X_b^{(i)}\theta  )\cdot(-X_2^{(i)})  \\
\vdots   \\
\sum\limits^m_{i=1}2(y^{(i)} - \mathbf X_b^{(i)}\theta  )\cdot(-X_n^{(i)})  \\
\end{pmatrix}
= 2
\begin{pmatrix}
\sum\limits^m_{i=1}(\mathbf X_b^{(i)}\theta  -y^{(i)})  \\
\sum\limits^m_{i=1}(\mathbf X_b^{(i)}\theta -y^{(i)})\cdot X_1^{(i)}  \\
\sum\limits^m_{i=1}(\mathbf X_b^{(i)}\theta -y^{(i)})\cdot X_2^{(i)}  \\
\vdots   \\
\sum\limits^m_{i=1}(\mathbf X_b^{(i)}\theta -y^{(i)})\cdot X_n^{(i)}  \\
\end{pmatrix}
$$
​	由于在上述求损失函数梯度公式中，每一项均是求和，所以，损失函数梯度的大小与 m 有关，这显然梯度大小与样本数量有关，这实际是不合理的，实际应该是和梯度无关的，所以将目标函数除以m，得：
$$
J(\theta) =  \frac{2}{m}\sum\limits^m_{i=1}(y^{(i)} - \hat y^{(i)} )^2
$$
​	即，<font color="red">$J(\theta) = MSE(y, \hat y)$</font> ，有时也取 $J(\theta) =  \frac{1}{2m}\sum\limits^m_{i=1}(y^{(i)} - \hat y^{(i)} )^2$ ，目的均是为了方便计算。

​	最终梯度公式变为：
$$
\nabla J(\theta) = 
\begin{pmatrix}
\frac{\partial J}{\partial \theta_0}   \\
\frac{\partial J}{\partial \theta_1}    \\
\frac{\partial J}{\partial \theta_2}    \\
\vdots   \\
\frac{\partial J}{\partial \theta_n}   \\
\end{pmatrix}
= \frac{2}{m}
\begin{pmatrix}
\sum\limits^m_{i=1}(\mathbf X_b^{(i)}\theta  -y^{(i)})  \\
\sum\limits^m_{i=1}(\mathbf X_b^{(i)}\theta -y^{(i)})\cdot X_1^{(i)}  \\
\sum\limits^m_{i=1}(\mathbf X_b^{(i)}\theta -y^{(i)})\cdot X_2^{(i)}  \\
\vdots   \\
\sum\limits^m_{i=1}(\mathbf X_b^{(i)}\theta -y^{(i)})\cdot X_n^{(i)}  \\
\end{pmatrix}
$$

### 3.2、线性回归梯度下降法

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
x = 2 * np.random.random(size=100)
y = x * 3. + 4. + np.random.normal(size=100)

X = x.reshape(-1, 1)

plt.scatter(x, y)
plt.show()
```

​	为了可视化的方便，这里 x 是使用的一维数组表示，同时为了方便拓展到多维，将 x 进行reshape 转换成 X 。

> X.shape
>
> > (100 , 1)
>
> y.shape
>
> > (100 ,)

​	数据图像：

![1563979764541](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563979764541.png)

​	

​	**使用梯度下降法训练：**

```python
# 求解损失函数
def J(theta, X_b , y):
    try:
        return np.sum((y - X_b.dot(theta))**2) / len(X_b)
    except:
        return float('inf')
    
# 求解损失函数梯度
def dJ(theta, X_b, y):
    res = np.empty(len(theta))   # 定义一个长度为len(theta)的空数组
    res[0] = np.sum(X_b.dot(theta) - y)  # 第一个元素单独求
    for i in range(1, len(theta)):       # 第1~n个元素单独求
        res[i] = (X_b.dot(theta) - y).dot(X_b[:,i])
    return res * 2 / len(X_b)

# 梯度下降求参数
def gradient_descent(X_b, y, initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
    
    theta = initial_theta
    cur_iter = 0

    while cur_iter < n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if(abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
            
        cur_iter += 1

    return theta
```

​	求解实现程序：

```python
X_b = np.hstack([ np.ones((len(x), 1)), x.reshape(-1,1) ])
initial_theta = np.zeros(X_b.shape[1])  # theta 初始化为 0
eta = 0.01

theta = gradient_descent(X_b, y, initial_theta, eta)
```

**输出 ：theta**

> > ```python
> > array([ 4.02145786,  3.00706277])
> > ```

​	

### 3.3、封装梯度下降法

​	封装梯度下降法代码：

```PYTHON
import numpy as np
from .metrics import r2_score

class LinearRegression:

    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None
        self.intercept_ = None
        self._theta = None


    def fit_gd(self, X_train, y_train, eta=0.01, n_iters=1e4):
        """根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        def J(theta, X_b, y):
            try:
                return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
            except:
                return float('inf')

        def dJ(theta, X_b, y):
            res = np.empty(len(theta))
            res[0] = np.sum(X_b.dot(theta) - y)
            for i in range(1, len(theta)):
                res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
            return res * 2 / len(X_b)

        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

            theta = initial_theta
            cur_iter = 0

            while cur_iter < n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
                    break

                cur_iter += 1

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self



    def __repr__(self):
        return "LinearRegression()"

```

​	测试封装后的代码实现：

```python
from myML.LinearRegression import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit_gd(X, y)
```

​	**输出：**

> > ```python
> > lin_reg.coef_  = array([ 3.00706277])
> > ```
>
> > ```python
> > lin_reg.intercept_  =  4.021457858204859
> > ```

