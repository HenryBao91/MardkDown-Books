# <center>梯度下降法</center>

## 1、梯度下降法

## 1.1、梯度下降

​		梯度下降是一种非常通用的<font color='red'>优化算法</font>，能够为大范围的问题寻找最优解。**梯度下降的中心思想就是：迭代地调整参数从而使成本函数最小化**。

特点：

- 梯度下降法不是一个机器学习算法

- 梯度下降法是一种基于搜索的最优化方法

- 作用：最小化一个损失函数

- 梯度上升法：最大化一个效用函数

  使用梯度下降方法的原因：**很多机器学习的模型是无法直接求到最优解**。

![1563885114058](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563885114058.png)

​		对于上图损失函数来说，在损失函数上任取一点（下图蓝点），可以得到该点的损失函数 $J$ 值和该点的导数。由数学知识可知，如果该点的导数不为0，则该点肯定不在函数的极值点上，导数通常可以写为$\frac{dJ}{d\theta}$。

![1563885900707](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563885900707.png)

​		对于该问题，导数代表 $\theta$ 变化时，$J$ 相应的变化。导数可以代表方向，对应 $J$ 增大的方向，由于导数是负值，所以 $J$ 增大的方向应该是在 $\theta$ 轴的负方向上，也就是 $\theta$ 减小的时候。对应的还需要一个  $\theta$ 移动的步长，即 $\eta$ ，梯度下降中一个重要参数是每一步的步长，这取决于超参数学习率。叫做梯度下降法的原因：对于一维的的可以直接求导表征，但是对于多维的需要求各个方向的导数，也就是函数的梯度。

## 1.2、梯度下降法关键点

### 1.2.1、学习率

​		对于 $\eta$ ：

-   $\eta$ 称为学习率（Learning rate）
-   $\eta$ 的取值影响获得最优解的速度
-   $\eta$ 取值不合适，甚至得不到最优解
-   $\eta$ 是梯度下降法的一个超参数

1. 学习率 $\eta$ 太小，则算法需要经过大量迭代才能收敛，这将耗费很长时间。

![1563886481781](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563886481781.png)



2. 学习率 $\eta$ 太高，则算法可能会发散，值越来越大，最后无法找到好的解决方案。

![1563886552844](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563886552844.png)



### 1.2.2、梯度陷阱

​		对于线性回归算法而言，线性回归模型的 MSE 成本函数恰好是一个凸函数，这意味着连接曲线上任意两个点的线段永远不会跟曲线相交，即不存在局部最小，只存在一个全局最小值。

![1563886936546](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563886936546.png)

​		**并不是对于所有的函数都有唯一的极值点**。

 解决方法：

- 多次运行，随机初始化点
- 梯度下降法的初始点也是一个超参数

### 1.2.3、特征值缩放

​		即使损失函数是凸函数，但如果不同特征的尺寸差别巨大，如下图，那它可能是一个非常细长的“碗”，图中左边训练集上特征1和特征2具有相同的数值规模，而右边的训练集上，特征1的值则比特征2小得多。

![1563887217478](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563887217478.png)

​		由此可见，左图的梯度下降法直接走向最小值，可以快速到达。而在右图中，先是沿着与全局最小值方向近乎垂直的方向前进，接下来是一段几乎平坦的尝尝的山谷，最终虽然也能够到达最小值，但是需要花费大量的训练时间。

​		<font color='darkblue'>**应用梯度下降法时，需要保证所有特征值的大小比例差不多，可以使用sklearn中的StandartScaler类，否则收敛的时间会很长。**</font>



## 2、梯度下降法模拟

### 2.1、梯度下降法实验

```python
import numpy as np
import matplotlib.pyplot as plt

plot_x = np.linspace(-1, 6, 141)
plot_y = (plot_x - 2.5)**2 - 1   # y 是 x 的简单二次函数

plt.plot(plot_x, plot_y) 
plt.show()
```

图像：

![1563887852853](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563887852853.png)

```python
# 函数导数：
def dJ(theta):
    return 2*(theta - 2.5)

# 损失函数：
def J(theta):
    return (theta - 2.5)**2 - 1

eta = 0.1
epsilon = 1e-8

theta = 0.0
while True:
    grandient = dJ(theta)  # 梯度
    last_theta = theta     # 当前 theta
    theta = theta - eta * grandient  # 新移动的 theta
    # 判断 theta 是否来到了导数最小值的点
    if( abs(J(theta) - J(last_theta)) < epsilon ):  
        break

print(theta)
print(J(theta))
```

> output:
>
> > ```python
> > 2.499891109642585
> > -0.99999998814289
> > ```

对 $\theta$ 的观察：

```python
theta = 0.0
theta_history = [theta]
while True:
    gradient = dJ(theta)
    last_theta = theta
    theta = theta - eta * gradient
    theta_history.append(theta)
    
    if(abs(J(theta) - J(last_theta)) < epsilon):
        break

plt.plot(plot_x, J(plot_x))
plt.plot(np.array(theta_history), J(np.array(theta_history)), color="r", marker='+')
plt.show() 
```

绘制图像：

![1563888291454](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563888291454.png)



### 2.2、梯度下降法封装

```python
theta_history = []

def gradient_descent(initial_theta, eta, epsilon=1e-8):
    theta = initial_theta
    theta_history.append(initial_theta)

    while True:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
    
        if(abs(J(theta) - J(last_theta)) < epsilon):
            break
            
def plot_theta_history():
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color="r", marker='+')
    plt.show()
```

1. 减小$\theta$ 观察输出效果：

```python
eta = 0.01
theta_history = []
gradient_descent(0, eta)
plot_theta_history()
```

​	绘制输出图像（$\theta$ 更加密集）：

![1563888530813](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563888530813.png)

2. 增大$\theta$ 观察输出效果：

```python
eta = 0.8
theta_history = []
gradient_descent(0, eta)
plot_theta_history()
```

​	绘制输出图像（$\theta$ 更加稀疏，幅度更大）：

![1563888697463](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563888697463.png)

3. 继续增大$\theta$ 观察输出效果：

```python
eta = 1.1   #  eta 设置的过大
theta_history = []
gradient_descent(0, eta)
plot_theta_history()
```

​	编译器报错：

![1563888766216](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563888766216.png)

修改损失函数和梯度下降法代码，避免报错：

```PYTHON
def J(theta):
    try:
        return (theta-2.5)**2 - 1.
    except:
        return float('inf')   # 进行异常检测，如果错误返回浮点数最大值
    
 #  n_iters 代表运行最大循环次数  
def gradient_descent(initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
    
    theta = initial_theta
    i_iter = 0
    theta_history.append(initial_theta)

    while i_iter < n_iters:    # 避免死循环
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
    
        if(abs(J(theta) - J(last_theta)) < epsilon):
            break
            
        i_iter += 1
        
    return    
```

重新执行上面代码：

```PYTH
eta = 1.1
theta_history = []
gradient_descent(0, eta, n_iters=10)
plot_theta_history()
```

输出图像：

![1563889011407](E:\MardkDown-Books\机器学习\梯度下降法\梯度下降法.assets\1563889011407.png)

