# <center>多项式回归</center>





![1567931991391](E:\MardkDown-Books\机器学习\多项式回归\Untitled.assets\1567931991391.png)

![1567932048307](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567932048307.png)





```python
import numpy as np 
import matplotlib.pyplot as plt

x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, 100)

plt.scatter(x, y)
plt.show()
```

![1567934664740](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567934664740.png)

用线性回归拟合：

```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_predict = lin_reg.predict(X)

plt.scatter(x, y)
plt.plot(x, y_predict, color='r')
plt.show()
```

![1567934697852](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567934697852.png)



```python
X2 = np.hstack([X, X**2])  # 添加一个特征
lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)

plt.scatter(x, y)
plt.plot(x, y_predict2, color='r')
plt.show()
# 因为 x 是乱序的，所以绘制的直线也是乱序的
```

![1567937374027](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567937374027.png)

```python
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()
```

![1567937399188](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567937399188.png)

```python
lin_reg2.coef_
>>> array([0.97540289, 0.51954067])

lin_reg2.intercept_
>>> 2.0724954416905828
```



## scikit-learn中的多项式回归和Pipeline

```python
import numpy as np 
import matplotlib.pyplot as plt

x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, 100)

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
poly.fit(X)
X2 = poly.transform(X)

#  X2[:5,:]
# 第一列1代表x的0次方特征，第二列代表x的1次方特征，以此类推
# array([[ 1.        ,  0.14960154,  0.02238062],
#       [ 1.        ,  0.49319423,  0.24324055],
#       [ 1.        , -0.87176575,  0.75997552],
#       [ 1.        , -1.33024477,  1.76955114],
#       [ 1.        ,  0.47383199,  0.22451675]])



from sklearn.linear_model import LinearRegression

lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()
```

![1567938296634](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567938296634.png)

```python
lin_reg2.coef_
>>> array([ 0.        ,  0.9460157 ,  0.50420543])

lin_reg2.intercept_
>>> 2.1536054095953823
```



### 关于PolynomialFeatures

![1567938981979](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567938981979.png)

多项式回归，degree 增加，生成的项将呈指数级增长。



### Pipeline

```python
# 准备数据
x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, 100)
```

pipeline

```python
# pipeline 实现
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

poly_reg = Pipeline([
    ("poly", PolynomialFeatures(degree=2)),  # 第一步
    ("std_scaler", StandardScaler()),        # 第二步
    ("lin_reg", LinearRegression())          # 第三步
])
```

训练和绘图：

```python
poly_reg.fit(X, y)
y_predict = poly_reg.predict(X)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()
```

![1567939244810](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567939244810.png)



## 过拟合和欠拟合

### 1、使用线性回归

```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.score(X, y)

>>> 0.49537078118650091
```

```python
y_predict = lin_reg.predict(X)
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()
```

![1567939933171](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567939933171.png)

 均方误差值：

```python
from sklearn.metrics import mean_squared_error

y_predict = lin_reg.predict(X)
mean_squared_error(y, y_predict)

>>> 3.0750025765636577
```



### 2、使用多项式回归

1、degree = 2

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])

poly2_reg = PolynomialRegression(degree=2)
poly2_reg.fit(X, y)
```

```
>>> Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('std_scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lin_reg', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])
```

均方误差值：

```python
y2_predict = poly2_reg.predict(X)
mean_squared_error(y, y2_predict)

>>>  1.0987392142417856
```

绘制曲线：

```python
plt.scatter(x, y)
plt.plot(np.sort(x), y2_predict[np.argsort(x)], color='r')
plt.show()
```

![1567941720497](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567941720497.png)

2、degree = 10

均方误差值：

```python
poly10_reg = PolynomialRegression(degree=10)
poly10_reg.fit(X, y)

y10_predict = poly10_reg.predict(X)
mean_squared_error(y, y10_predict)

>>> 1.0508466763764164
```

绘制曲线：

```python
plt.scatter(x, y)
plt.plot(np.sort(x), y10_predict[np.argsort(x)], color='r')
plt.show()
```

![1567941795882](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567941795882.png)

3、degree = 100

均方误差值：

```python
poly100_reg = PolynomialRegression(degree=100)
poly100_reg.fit(X, y)

y100_predict = poly100_reg.predict(X)
mean_squared_error(y, y100_predict)

>>>  0.68743577834336944
```

绘制曲线：

```python
plt.scatter(x, y)
plt.plot(np.sort(x), y100_predict[np.argsort(x)], color='r')
plt.show()
```

![1567941840644](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567941840644.png)

在 [-3 , 3] 等间隔生成数据点绘制曲线：

该曲线能较为准确的描绘 degree = 100 时预测的结果。

```python
X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
y_plot = poly100_reg.predict(X_plot)

plt.scatter(x, y)
plt.plot(X_plot[:,0], y_plot, color='r')
plt.axis([-3, 3, 0, 10])
plt.show()
```

![1567941926497](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567941926497.png)

很显然，当 degree 值越高，拟合程度越高，但是从均方误差的角度看是变好的，但是并不能真实实际的预测样本结果，这种情况就是**过拟合**；对于最开始直接使用线性回归来拟合预测，效果太差，这种情况是模型过于简单，称为**欠拟合**。



过拟合导致的结果是对新来的要预测的数据的泛化能力差，也就是说训练模型的目的不是为了很大程度的拟合训练集，而是为了在面对测试数据的时候能够有很好的预测能力。 ![1567942324950](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567942324950.png)

 

### train test split的意义

1、普通线性回归

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

lin_reg.fit(X_train, y_train)
y_predict = lin_reg.predict(X_test)
mean_squared_error(y_test, y_predict)
```

> ```python
> 2.2199965269396573
> ```



2、普多项式回归

```python
# 2阶多项式模型

poly2_reg.fit(X_train, y_train)
y2_predict = poly2_reg.predict(X_test)
mean_squared_error(y_test, y2_predict)
```

> ```python
> 0.80356410562978997
> ```

```python
# 10阶多项式模型

poly10_reg.fit(X_train, y_train)
y10_predict = poly10_reg.predict(X_test)
mean_squared_error(y_test, y10_predict)
```

> ```python
> 0.92129307221507939
> ```

由上可以观察出，10阶多项式回归模型在测试集上得到的均方误差是比2阶多项式回归模型在测试集上得到的均方误差是要高的，说明2阶多项式回归模型在面对测试数据集时泛化能力更好。

```python
# 100阶多项式模型

poly100_reg.fit(X_train, y_train)
y100_predict = poly100_reg.predict(X_test)
mean_squared_error(y_test, y100_predict)
```

> ```python
> 14075796419.234262
> ```

进一步，当采用100阶多项式回归时，模型在测试集上的均方误差值更大，泛化能力更差。



测试数据集的意义

![1567946888171](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567946888171.png)



![1567946629940](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567946629940.png)



## 学习曲线



```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)

plt.scatter(x, y)
plt.show()
```

![1567947122887](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567947122887.png)



```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)

X_train.shape
>>> (75, 1)
```

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

train_score = []
test_score = []
for i in range(1, 76):
    lin_reg = LinearRegression()
    lin_reg.fit(X_train[:i], y_train[:i])
    
    y_train_predict = lin_reg.predict(X_train[:i])
    train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    
    y_test_predict = lin_reg.predict(X_test)
    test_score.append(mean_squared_error(y_test, y_test_predict))
```

绘制曲线：

```python
plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
plt.legend()
plt.show()
```

![1567947296527](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567947296527.png)

将上述绘图函数封装：

```python
def plot_learning_curve(algo, X_train, X_test, y_train, y_test):
    train_score = []
    test_score = []
    for i in range(1, len(X_train)+1):
        algo.fit(X_train[:i], y_train[:i])
    
        y_train_predict = algo.predict(X_train[:i])
        train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    
        y_test_predict = algo.predict(X_test)
        test_score.append(mean_squared_error(y_test, y_test_predict))
        
    plt.plot([i for i in range(1, len(X_train)+1)], 
                               np.sqrt(train_score), label="train")
    plt.plot([i for i in range(1, len(X_train)+1)], 
                               np.sqrt(test_score), label="test")
    plt.legend()
    plt.axis([0, len(X_train)+1, 0, 4])
    plt.show()
    
plot_learning_curve(LinearRegression(), X_train, X_test, y_train, y_test)
```

使用多项式回归：

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])

poly2_reg = PolynomialRegression(degree=2)
plot_learning_curve(poly2_reg, X_train, X_test, y_train, y_test)
```

![1567953948845](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567953948845.png)

使用2阶多项式回归最后稳定在误差为1.2左右，使用线性回归最终稳定在2.0左右，说明2阶多项式回归的效果更好，线性回归则是欠拟合。

使用20阶多项式回归：

```python
poly20_reg = PolynomialRegression(degree=20)
plot_learning_curve(poly20_reg, X_train, X_test, y_train, y_test)
```

![1567954064557](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567954064557.png)

有图可知，模型对测试集的评分效果并不好，即出现了过拟合的情况。

![1567954230679](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567954230679.png)





![1567954562432](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567954562432.png)





交叉验证

![1567954714152](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567954714152.png)

1、准备数据

```python
import numpy as np
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target
```

2、测试train_test_split

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=666)

from sklearn.neighbors import KNeighborsClassifier

best_k, best_p, best_score = 0, 0, 0
for k in range(2, 11):
    for p in range(1, 6):
        knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score > best_score:
            best_k, best_p, best_score = k, p, score
            
print("Best K =", best_k)
print("Best P =", best_p)
print("Best Score =", best_score)
```

输出结果：

![1567955314484](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567955314484.png)

3、使用交叉验证

```python
from sklearn.model_selection import cross_val_score

knn_clf = KNeighborsClassifier()
cross_val_score(knn_clf, X_train, y_train)
```

> ```python
> array([ 0.98895028,  0.97777778,  0.96629213])
> ```

```python
best_k, best_p, best_score = 0, 0, 0
for k in range(2, 11):
    for p in range(1, 6):
        knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=k, p=p)
        scores = cross_val_score(knn_clf, X_train, y_train)   # 使用 cv 验证调参
        score = np.mean(scores)
        if score > best_score:
            best_k, best_p, best_score = k, p, score
            
print("Best K =", best_k)
print("Best P =", best_p)
print("Best Score =", best_score)
```

![1567955417516](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567955417516.png)

使用最佳参数进行分类：

```python
best_knn_clf = KNeighborsClassifier(weights="distance", n_neighbors=2, p=2)
best_knn_clf.fit(X_train, y_train)
best_knn_clf.score(X_test, y_test)
```

> ```python
> 0.98052851182197498
> ```

使用网格搜索：

```python
from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        'weights': ['distance'],
        'n_neighbors': [i for i in range(2, 11)], 
        'p': [i for i in range(1, 6)]
    }
]

grid_search = GridSearchCV(knn_clf, param_grid, verbose=1)
grid_search.fit(X_train, y_train)
```

```python
grid_search.best_score_
```

> ```python
> 0.98237476808905377
> ```

```python
grid_search.best_params_
```

> ```python
> {'n_neighbors': 2, 'p': 2, 'weights': 'distance'}
> ```

```python
best_knn_clf = grid_search.best_estimator_
best_knn_clf.score(X_test, y_test)
```

> ```python
> 0.98052851182197498
> ```



### cv参数

```python
cross_val_score(knn_clf, X_train, y_train, cv=5)
```

> ```python
> array([ 0.99543379,  0.96803653,  0.98148148,  0.96261682,  0.97619048])
> ```

```python
# 网格搜索方法
grid_search = GridSearchCV(knn_clf, param_grid, verbose=1, cv=5)
```

![1567955688822](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567955688822.png)



![1567955718688](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1567955718688.png)



![1568039726558](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568039726558.png)





![1568039750278](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568039750278.png)

![1568039828220](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568039828220.png)



偏差：

![1568040775666](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568040775666.png)

方差：

![1568040827451](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568040827451.png)





![1568040835913](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568040835913.png)

![1568040946805](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568040946805.png)

![1568040988103](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568040988103.png)

 

![1568041204588](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568041204588.png)





![1568041240517](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568041240517.png)





![1568041343751](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568041343751.png)



注意，$\theta_0$ 不在正则项中，因为 $\theta_0$ 代表的是直线的截距，决定了曲线的高低，但是不决定曲线的高低、缓和程度。另外 $\alpha$ 也是一个超参数， $\alpha$ 代表在模型正则化下新的损失函数中，要让每一个 $\theta$ 都尽可能的小，这个小的程度占整个新的损失函数的多少，即 $\alpha=0$ 时，正则化项相当于没有作用，反之 $\alpha=+∞$ 时，相当于只有正则化项起作用。

上述模型正则化方式又叫做岭回归。



## 岭回归 Ridge Regression

```Python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x + 3 + np.random.normal(0, 1, size=100)

plt.scatter(x, y)
plt.show()
```

![1568119903055](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568119903055.png)

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def PolynomialRegression(degree):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lin_reg", LinearRegression())
    ])

from sklearn.model_selection import train_test_split

np.random.seed(666)
X_train, X_test, y_train, y_test = train_test_split(X, y)

from sklearn.metrics import mean_squared_error

poly_reg = PolynomialRegression(degree=20)
poly_reg.fit(X_train, y_train)

y_poly_predict = poly_reg.predict(X_test)
mean_squared_error(y_test, y_poly_predict)
```

> ```python
> 167.94010867293571
> ```

```python
X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
y_plot = poly_reg.predict(X_plot)

plt.scatter(x, y)
plt.plot(X_plot[:,0], y_plot, color='r')
plt.axis([-3, 3, 0, 6])
plt.show()
```

![1568119965445](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568119965445.png)

将绘制模型函数封装：

```python
def plot_model(model):
    X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
    y_plot = model.predict(X_plot)

    plt.scatter(x, y)
    plt.plot(X_plot[:,0], y_plot, color='r')
    plt.axis([-3, 3, 0, 6])
    plt.show()

plot_model(poly_reg)
```



### 使用岭回归

```python
from sklearn.linear_model import Ridge

def RidgeRegression(degree, alpha):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("ridge_reg", Ridge(alpha=alpha))
    ])

ridge1_reg = RidgeRegression(20, 0.0001)
ridge1_reg.fit(X_train, y_train)

y1_predict = ridge1_reg.predict(X_test)
mean_squared_error(y_test, y1_predict)
```

> ```python
> 1.3233492754051845
> ```

```python
plot_model(ridge1_reg)
```



![1568120070318](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568120070318.png)



```python
ridge2_reg = RidgeRegression(20, 1)
ridge2_reg.fit(X_train, y_train)

y2_predict = ridge2_reg.predict(X_test)
mean_squared_error(y_test, y2_predict)
```

> ```python
> 1.1888759304218448
> ```

```python
plot_model(ridge2_reg)
```

![1568121630493](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568121630493.png)



```python
ridge3_reg = RidgeRegression(20, 100)
ridge3_reg.fit(X_train, y_train)

y3_predict = ridge3_reg.predict(X_test)
mean_squared_error(y_test, y3_predict)
```

> ```python
> 1.3196456113086197
> ```

```python
plot_model(ridge3_reg)
```

![1568121710956](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568121710956.png)



由上结果可以看到，均方误差又增大了，说明正则化的作用过于强烈，可以进一步增大正则化的作用进行查看极限情况：



```python
ridge4_reg = RidgeRegression(20, 10000000)
ridge4_reg.fit(X_train, y_train)

y4_predict = ridge4_reg.predict(X_test)
mean_squared_error(y_test, y4_predict)
```

> ```python
> 1.8408455590998372
> ```

```python
plot_model(ridge4_reg)
```

![1568121803994](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568121803994.png)

此时均分误差更大了一些，但是还是远远小于过拟合时的均方误差。图中模型绘制出的曲线是一条直线，即当 $\alpha$ 非常大的时候，相当于本质就是在优化正则化项 ，也就是说让所有 $\theta_i$ 的平方和尽可能的小，则对应的最小值也就是所有 $\theta_i=0$ 的时候， 最终的结果就是一条平行于 $x$ 轴的直线，没有任何斜率。



## LASSO



![1568122514431](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568122514431.png)



 ````python
from sklearn.linear_model import Lasso

def LassoRegression(degree, alpha):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("lasso_reg", Lasso(alpha=alpha))
    ])

 ````

```python
lasso1_reg = LassoRegression(20, 0.01)
lasso1_reg.fit(X_train, y_train)

y1_predict = lasso1_reg.predict(X_test)
mean_squared_error(y_test, y1_predict)
```

> ```python
> 1.1496080843259966
> ```

```python
plot_model(lasso1_reg)
```

![1568123214750](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568123214750.png)





```python
lasso2_reg = LassoRegression(20, 0.1)
lasso2_reg.fit(X_train, y_train)

y2_predict = lasso2_reg.predict(X_test)
mean_squared_error(y_test, y2_predict)
```

> ```python
> 1.1213911351818648
> ```

```python
plot_model(lasso2_reg)
```

![1568123358281](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568123358281.png)





```python
lasso3_reg = LassoRegression(20, 1)
lasso3_reg.fit(X_train, y_train)

y3_predict = lasso3_reg.predict(X_test)
mean_squared_error(y_test, y3_predict)
```

> ```python
> 1.8408939659515595
> ```

```python
plot_model(lasso3_reg)
```

![1568123519650](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568123519650.png)





Ridge 和 Lasso 的比较

![1568123794743](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568123794743.png)



![1568123890087](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568123890087.png)





![1568124028470](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568124028470.png)



Ridge 中，在 $\alpha$ 趋于 0 的过程中是缓慢变化的，也就是一直是有值，不为 0 的。

![1568124067376](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568124067376.png)





![1568124633084](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568124633084.png)

绝对值没有导数，但是可以通过符号函数的方式进行考虑，所以对于 Lasso 中正则化项的梯度相当于会沿着某些轴先到 0 ，最终全部归为 0 的过程，所以在 Lasso 中，存在很多为 0 的系数项。

![1568124668853](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568124668853.png)



![1568124876419](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568124876419.png)



![1568125196881](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568125196881.png)



![1568125190562](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568125190562.png)





![1568125236591](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568125236591.png)



![1568125364860](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568125364860.png)



![1568125421242](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568125421242.png)



![1568125785347](E:\MardkDown-Books\机器学习\多项式回归\多项式回归.assets\1568125785347.png)

实际应用中应优先考虑岭回归，当特征项特别多的时候，由于岭回归不具有特征选择的作用（即某些特征项系数为0），那么就考虑使用弹性网，因为它同时结合了 Lasso回归 和 岭回归 的作用。

 